---
title: "Machine Learning - Predicting Survival on the Titanic"
author: Sylvia Lee(sylvia19) and Patrick Tung(ptung)
output: github_document
date: 15 Nov, 2018
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_knit$set(root.dir = here::here())
library(here)
library(imager)
```

### Table of Contents
1) Introduction
2) Usage
3) Dependencies
4) Exploratory Visualizations
5) Prediction
   + Predictions
   + Feature Importance Ranking
6) Limitations

### Introduction

*Who will survive through the Titanic disaster?*

For most people, "Titanic" is both a classic movie and a beautiful love story. However, the infamous Titanic catastrophe had also been said to be a prime example of social stratification and status discriminations in the 1900s. In addition to the "women and children first" evacuation policy, it had been rumored that the lives of the people with social prestige and high class standing were prioritized in the momment of danger. In this analysis, we used supervised machine learning (ML) to answer the question **"What are the 3 strongest predictors of people who survived on the Titanic?"**

We retrieved the data from [Kaggle's Titanic:Machine Learning from Disaster](https://www.kaggle.com/c/titanic) and developed a decision-classification-tree machine learning model focusing on following features:

- Passenger class
- Sex
- Age
- Number of siblings/spouses onboard
- Number of parents/children onboard
- Fare price

In our project, we explored the dataset by generating graphs of the features' distribution in the population of passengers. Subsequently we developed the decision tree model using Python's scikit-learn package and applied the model to a test dataset to predict the survival of the passenger given the same list of features. Lastly, we summarized our analysis by calculating the accuracy of our ML model and ranking the list of features' predictive power.


### Exploratory Analysis

The RMS Titanic only carried enough life boats for one third of the passengers, and our data was reflective of this. The data showed disproportionately larger proportion of passengers that did not survive. Thus, we compared the feature distributions within the desinated groups, the "survived "and the "did not survive". We plotted each feature according to the passenger's survival status. Which allowed us to gain a sense of the differential distribution of features depending on the passenger's survival.

In general we found that the data reflected the "women and children first" evacuation policy. There seemed to be larger proportion of women and children that survived than those the did not. Interestingly, we found that there were indeed larger proportion of survived passengers that had the features of "first class passenger" and "paid high fare price". On the other hand, family size (number of parent, children, siblings and spouse) did not appear to cause large difference



**Age**
```{r age_fig ,fig.align = "center"}

knitr::include_graphics(here::here("results/figure/Age_plot.png"))
```

> <center>
> Fig 1. Histograms of ages among the passengers that survived (left) and did not survive (right).
> </center>

The shape of the distributions were very similar. Both ranges from 0 to ~70 years old and had a single peak at approximately 20 to 30 years old.  However, as expected from the "women and children first" evacuation policy, the "survived" distribution had higher frequencies in the age range of 0 to 10 than the "did not survive" group.


**Sex**
```{r sex_fig, fig.align = "center"}

knitr::include_graphics(here::here("results/figure/sex.png"))

```

> <center>
> Fig 2. Bar plot of sex distribution among the passengers that survived versus those did not survive.
> </center>

This figure cohered with our expectation that there would be more females that survived the catastrophe. In fact the number of survived females more than doubled the number of females that did not survive. In contrast, the number of males that survived was five times less than the number of males that did not survive.


**Passenger Class**

```{r Pclass_fig, fig.align = "center"}

knitr::include_graphics(here::here("results/figure/pclass.png"))
```

> <center>
> Fig 3. Bar plot of passenger class distribution among the passengers that survived versus those did not survive.
> </center>

There seemed to be a pattern in the survival proportion of the passenger classes. In the first class, larger proportion of passenger survived. In the second class, there was approximately equal proportion of passenger between those that survived and those that did not. In the third class, the proportion of those that did not survive was three times larger than the proportion that survived.

**Fare Price**

```{r fare_fig, fig.align = "center"}

knitr::include_graphics(here::here("results/figure/Fare_plot.png"))
```

> <center>
> Fig 4. Histograms of fare prices paid by the passengers that survived (left) and did not survive (right).
> </center>


**Number of Parents/Children Onboard**

```{r parch_fig, fig.align = "center"}

knitr::include_graphics(here::here("results/figure/Parch_plot.png"))
```

> <center>
> Fig 5. Histograms of number of parent or children that was onboard with the passengers that did survive (left) and did not survive (right).
> </center>

**Number of Siblings/Spouses Onboard**

```{r sibsp_fig, fig.align = "center"}

knitr::include_graphics(here::here("results/figure/SibSp_plot.png"))
```

> <center>
> Fig 6. Histograms of number of siblings or spouse that was onboard with the passengers that did survive (left) and did not survive (right).
> </center>


### Predictions and Evaluations

**Predictions**

Below, is a snippet of our predictions for both the training data set and the testing data set:
```{r echo=FALSE}
train_prediction <- read.csv(here::here("results/train_prediction.csv"))
test_prediction <- read.csv(here::here("results/test_prediction.csv"))
knitr::kable(head(train_prediction, 10))
knitr::kable(head(test_prediction, 10))
```

**Model Performance**

To evaluate the accuracy of the model, we used our testing data, which was not part of the training model. We then used this accuracy score to compare with the accuracy score we achieved from the training model. What we are trying to see here is whether or not there is an increase in the accuracy of our testing model to training model. We do not want our model to be overfit and therefore, cause poor generalization issues.

The accuracies we obtained are presented in the following table:

```{r echo=FALSE}
accuracies <- read.csv(here::here("results/accuracies.csv"))
knitr::kable(accuracies)
```
As you can see, our model predicted the training data set with an accuracy of 0.7778, predicted the testing data set with an accuracy of 0.8421. It would seem that our model is quite generalized for our prediction and therefore, we obtained a much higher accuracy score on the testing than on the training data set.

**Feature Importance Ranking**

The ultimate goal of our research is to determine which variables in the data set are among the most important. To do this, we took our tree and generated an importance score. The importance score evaluated by sklearn is the "gini importance", also known as "mean decrease impurity". Essentially, the higher the value here, the more important the feature is. You can see how our features are ranked below.

```{r echo=FALSE}
rank <- read.csv(here::here("results/feature_ranks.csv"))
knitr::kable(rank)
```
From our results, we can determine that the top three most important features in our model is: 1) Sex, 2) Passenger Class, 3) Fare Prices.


### Limitations

First of all, the biggest limitation to our project is that we only chose one type of model, which is decision tree. I believe that we could have used tested out different models for making our predictions, however, because we have not really learned a lot of models, we were wary of fitting something that we were unfamiliar with. Therefore, to compensate for having choosing only one model, we decided to use cross validation to pick the best hyperparameter for our decision tree. We performed cross validation on our tree so that we could choose the best `max_depth`.

Since we are on the topic of cross validation, another limitation that we encountered during this project was that we used means and medians for the imputation of missing values. We could have decided to use regressors to make predictions on the best value to be inserted into the missing values. However, this was somewhat beyond our current understanding, so we decided that using the means and medians were sufficient for our project.

Lastly, for our prediction, we decided to subset the data set to only the relevant features that we were looking for in our research question. The entire data set that we were given had more features
